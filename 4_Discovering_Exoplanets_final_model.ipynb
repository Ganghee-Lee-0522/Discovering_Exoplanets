{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUwXplkqMuI93R5OwnAJxL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59KG_n0D423d",
        "outputId": "f083ea1b-cb97-4e72-8497-b2a442106df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  exoplanets.csv.zip\n",
            "  inflating: exoplanets.csv          \n",
            "  inflating: __MACOSX/._exoplanets.csv  \n",
            "Validation Accuracy: 0.9052\n",
            "Validation Accuracy: 0.8276\n",
            "Validation Accuracy: 0.8534\n",
            "Validation Accuracy: 0.8534\n",
            "Validation Accuracy: 0.8534\n",
            "\n",
            "=== Cross Validation Results ===\n",
            "Mean Validation Accuracy: 0.8586\n",
            "Validation Accuracy Standard Deviation: 0.0253\n",
            "\n",
            "=== Final Test Set Evaluation ===\n",
            "Accuracy: 0.875\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.89      0.93        73\n",
            "           1       0.38      0.71      0.50         7\n",
            "\n",
            "    accuracy                           0.88        80\n",
            "   macro avg       0.68      0.80      0.71        80\n",
            "weighted avg       0.92      0.88      0.89        80\n",
            "\n",
            "\n",
            "=== Confusion Matrix ===\n",
            "[[65  8]\n",
            " [ 2  5]]\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "\n",
            "=== Best Parameters from Randomized Search ===\n",
            "{'subsample': 0.8, 'reg_lambda': 0.5, 'reg_alpha': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.6}\n",
            "\n",
            "=== Final Test Set Evaluation with Best Parameters ===\n",
            "Accuracy: 0.875\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.89      0.93        73\n",
            "           1       0.38      0.71      0.50         7\n",
            "\n",
            "    accuracy                           0.88        80\n",
            "   macro avg       0.68      0.80      0.71        80\n",
            "weighted avg       0.92      0.88      0.89        80\n",
            "\n",
            "\n",
            "=== Confusion Matrix ===\n",
            "[[65  8]\n",
            " [ 2  5]]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.fft import fft, fftfreq\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set()\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install -q --upgrade xgboost\n",
        "    !wget -q https://raw.githubusercontent.com/rickiepark/handson-gb/main/Chapter07/exoplanets.csv.zip\n",
        "\n",
        "!unzip -o exoplanets.csv.zip\n",
        "\n",
        "xgb.set_config(verbosity=0)\n",
        "\n",
        "df = pd.read_csv('exoplanets.csv', nrows=400)\n",
        "\n",
        "# 데이터 로드 및 준비\n",
        "X = df.filter(like=\"FLUX\")  # FLUX로 시작하는 열\n",
        "y = df[\"LABEL\"] - 1  # 타깃 변수\n",
        "\n",
        "# 1. 데이터 분할: 테스트 데이터 분리 (최종 평가용, 전체 데이터의 20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# 2. FFT 수행 및 진폭 계산\n",
        "timestep = 1\n",
        "fft_train_data = []\n",
        "fft_test_data = []\n",
        "\n",
        "for i in range(X_train.shape[0]):\n",
        "    sample_flux = X_train.iloc[i, :]  # i번째 샘플\n",
        "    n = len(sample_flux)\n",
        "\n",
        "    # FFT 수행\n",
        "    fft_result = fft(sample_flux.to_numpy())\n",
        "    magnitude = np.abs(fft_result[:n // 2])  # 양의 주파수 성분만 사용\n",
        "    fft_train_data.append(magnitude)\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    sample_flux = X_test.iloc[i, :]  # i번째 샘플\n",
        "    n = len(sample_flux)\n",
        "\n",
        "    # FFT 수행\n",
        "    fft_result = fft(sample_flux.to_numpy())\n",
        "    magnitude = np.abs(fft_result[:n // 2])  # 양의 주파수 성분만 사용\n",
        "    fft_test_data.append(magnitude)\n",
        "\n",
        "# FFT 결과를 DataFrame으로 변환\n",
        "fft_train_df = pd.DataFrame(fft_train_data)\n",
        "fft_test_df = pd.DataFrame(fft_test_data)\n",
        "\n",
        "# 3. 데이터 샘플링: SMOTE + Tomek Links 사용 (훈련 데이터에 대해서만 적용)\n",
        "smote_tomek = SMOTETomek(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(fft_train_df, y_train)\n",
        "\n",
        "# 4. 데이터 스케일링 (훈련-검증 데이터에 대해서만 스케일링 적용)\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(fft_test_df)\n",
        "\n",
        "# PCA 수행 및 주성분 선택\n",
        "pca = PCA(n_components=10)\n",
        "principal_components = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "# PCA 결과를 DataFrame으로 변환하고 원하는 주성분 선택\n",
        "pca_df = pd.DataFrame(principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
        "X_train_pca_selected = pca_df[['PC1', 'PC2', 'PC4', 'PC8', 'PC5']]  # 임의로 선택한 주성분 사용\n",
        "# X_train_pca_selected = pca_df[['PC1', 'PC2', 'PC5', 'PC9', 'PC10']]\n",
        "\n",
        "# 테스트 데이터에도 동일한 PCA 변환 적용 후 원하는 주성분 선택\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "X_test_pca_df = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
        "X_test_pca_selected = X_test_pca_df[['PC1', 'PC2', 'PC4', 'PC8', 'PC5']]  # 임의로 선택한 주성분 사용\n",
        "# X_test_pca_selected = X_test_pca_df[['PC1', 'PC2', 'PC5', 'PC9', 'PC10']]\n",
        "\n",
        "# 5. Stratified K-Fold Cross Validation 설정 (훈련-검증 데이터만 사용)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 모델 정의\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Cross Validation 학습 및 평가\n",
        "validation_scores = []\n",
        "\n",
        "for train_index, val_index in skf.split(X_train_pca_selected, y_train_resampled):\n",
        "    # 훈련-검증 데이터 분할\n",
        "    X_train, X_val = X_train_pca_selected.iloc[train_index], X_train_pca_selected.iloc[val_index]\n",
        "    y_train, y_val = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
        "\n",
        "    # 모델 학습\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 검증 데이터로 성능 평가\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    validation_scores.append(val_accuracy)\n",
        "\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# 교차 검증 평균 성능 출력\n",
        "print(\"\\n=== Cross Validation Results ===\")\n",
        "print(f\"Mean Validation Accuracy: {np.mean(validation_scores):.4f}\")\n",
        "print(f\"Validation Accuracy Standard Deviation: {np.std(validation_scores):.4f}\")\n",
        "\n",
        "# 6. 최종 모델 학습 및 테스트 평가\n",
        "model.fit(X_train_pca_selected, y_train_resampled)  # 전체 훈련-검증 데이터로 학습\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "y_test_pred = model.predict(X_test_pca_selected)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\n=== Final Test Set Evaluation ===\")\n",
        "print(\"Accuracy:\", test_accuracy)  # 테스트 세트 정확도 출력\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))  # 상세 성능 출력\n",
        "\n",
        "# 혼동 행렬 출력\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"\\n=== Confusion Matrix ===\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# 7. 하이퍼파라미터 튜닝을 위한 추가 코드\n",
        "# RandomizedSearchCV 설정\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2, 0.4],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'reg_alpha': [0, 0.1, 1],  # L1 regularization\n",
        "    'reg_lambda': [0.5, 1, 5]  # L2 regularization\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 하이퍼파라미터 최적화 수행\n",
        "random_search.fit(X_train_pca_selected, y_train_resampled)\n",
        "\n",
        "# 최적의 하이퍼파라미터 및 최적의 모델로 테스트 데이터 평가\n",
        "print(\"\\n=== Best Parameters from Randomized Search ===\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# 최적 모델로 테스트 평가\n",
        "best_model = random_search.best_estimator_\n",
        "y_test_pred = best_model.predict(X_test_pca_selected)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"\\n=== Final Test Set Evaluation with Best Parameters ===\")\n",
        "print(\"Accuracy:\", test_accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
        "\n",
        "# 혼동 행렬 출력\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"\\n=== Confusion Matrix ===\")\n",
        "print(conf_matrix)"
      ]
    }
  ]
}